{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b32d36-6682-4188-abe1-fc3445121385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#The Probability Mass function is defined on all the values of R, where it takes all the arguments of any real number. It doesn’t belong to the value of X when the argument value equals to zero and when the argument belongs to x, the value of PMF should be positive.\n",
    "\n",
    "#The probability mass function is usually the primary component of defining a discrete probability distribution, but it differs from the probability density function (PDF) where it produces distinct outcomes. This is the reason why probability mass function is used in computer programming and statistical modelling. In other words, probability mass function is a function that relates discrete events to the probabilities associated with those events occurring. The word “mass“ indicates the probabilities that are concentrated on discrete events.'''\n",
    "#The Probability Density Function(PDF) defines the probability function representing the density of a continuous random variable lying between a specific range of values. In other words, the probability density function produces the likelihood of values of the continuous random variable. Sometimes it is also called a probability distribution function or just a probability function. However, this function is stated in many other sources as the function over a broad set of values. Often it is referred to as cumulative distribution function or sometimes as probability mass function(PMF). However, the actual truth is PDF (probability density function ) is defined for continuous random variables, whereas PMF (probability mass function) is defined for discrete random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193cd735-5d63-48db-bae6-f9244bd55370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2The PMF is one way to describe the distribution of a discrete random variable. As we will see later on, PMF cannot be defined for continuous random variables. The cumulative distribution function (CDF) of a random variable is another method to describe the distribution of random variables. The advantage of the CDF is that it can be defined for any kind of random variable (discrete, continuous, and mixed).\n",
    "# i toss a coin twice then we add the probability of head one by one\n",
    "#The advantage of the CDF is that it can be defined for any kind of random variable (discrete, continuous, and mixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0cd791-cb6c-4ba3-b5e2-d5f889439a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3The normal distribution, also known as the Gaussian distribution, is the most important probability distribution in statistics for independent, random variables. Most people recognize its familiar bell-shaped curve in statistical reports.\n",
    "\n",
    "#The normal distribution is a continuous probability distribution that is symmetrical around its mean, most of the observations cluster around the central peak, and the probabilities for values further away from the mean taper off equally in both directions. Extreme values in both tails of the distribution are similarly unlikely. While the normal distribution is symmetrical, not all symmetrical distributions are normal. For example, the Student’s t, Cauchy, and logistic distributions are symmetric.\n",
    "\n",
    "#As with any probability distribution, the normal distribution describes how the values of a variable are distributed. It is the most important probability distribution in statistics because it accurately describes the distribution of values for many natural phenomena. Characteristics that are the sum of many independent processes frequently follow normal distributions. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distributio'''\n",
    "#The two main parameters of a (normal) distribution are the mean and standard deviation. The parameters determine the shape and probabilities of the distribution. The shape of the distribution changes as the parameter values change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e418e7-85b7-4bf5-ac91-fc946fc3c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 A Bernoulli distribution is a discrete probability distribution for a Bernoulli trial — a random experiment that has only two outcomes (usually called a “Success” or a “Failure”). For example, the probability of getting a heads (a “success”) while flipping a coin is 0.5. The probability of “failure” is 1 – P (1 minus the probability of success, which also equals 0.5 for a coin toss). It is a special case of the binomial distribution for n = 1. In other words, it is a binomial distribution with a single trial (e.g. a single coin toss).\n",
    "#A random variables that follows a Bernoulli distribution can only take on two possible values, but a random variable that follows a Binomial distribution can take on several values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb1b0b-6200-46f1-a5a7-e07228618cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 mean - data point/std \n",
    "# 50-60/10=-10\n",
    "#-1 acd to zcore date -1= .15866\n",
    "# so the probabilty of random selcet observation = 1-0.15866\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409d0e03-481b-4159-a287-6e236c962ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Uniform distribution is of two forms – discrete and continuous. This bifurcation depends on the type of outcomes with possibilities of occurrence. A discrete uniform distribution is the probability distribution where the researchers have a predefined number of equally likely outcomes. For example, when rolling dice, players are aware that whatever the outcome would be, it would range from 1-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7af0e9-856e-4ea0-a873-064930479fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 Z-score is a statistical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean. If a Z-score is 0, it indicates that the data point's score is identical to the mean score. A Z-score of 1.0 would indicate a value that is one standard deviation from the mean. Z-scores may be positive or negative, with a positive value indicating the score is above the mean and a negative score indicating it is below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1fc2c8-8957-46c2-9b12-6f5398a9bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9The Central Limit Theorem (CLT) is a statistical theory that states that the sampling distribution of the mean approaches a normal distribution as the sample size increases.01 This holds especially true for sample sizes over 30. As the sample size increases, the sample mean and standard deviation will be closer in value to the population mean and standard deviation.0 The CLT is a crucial pillar of statistics and machine learning and is at the heart of hypothesis testing.1 It is often used in conjunction with the law of large numbers, which states that the average of the sample means and standard deviations will come closer to equaling the population mean and standard deviation as the sample size grows.2 The CLT does not apply to distributions with infinite variance, such as the Cauchy distribution.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17390e-4bde-4aaa-9b69-33f0af84a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10The Central Limit Theorem (CLT) is a statistical theory that states that the sampling distribution of a sample variable approximates a normal distribution as the sample size becomes larger, regardless of the population's actual distribution shape.0 The CLT applies even to binomial populations provided that the minimum of np and n(1-p) is at least 5, where \"n\" refers to the sample size, and \"p\" is the probability of \"success\" on any given trial. The CLT holds true regardless of whether the source population is normal or skewed, provided the sample size is sufficiently large (usually n > 30). The theorem doesn't apply to distributions with infinite variance, such as the Cauchy distribution.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
